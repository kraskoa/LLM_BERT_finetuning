Could not locate the best model at ./results/checkpoint-500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
[34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
Test accuracy: 0.6730
Computers & Internet: 140000
Sports: 140000
Health: 140000
Business & Finance: 140000
Science & Mathematics: 140000
Entertainment & Music: 140000
Education & Reference: 140000
Family & Relationships: 140000
Politics & Government: 140000
Society & Culture: 140000
Dataset size: 1460000
Train dataset size: 95.89%
Test dataset size: 4.11%
Society & Culture: 2000
Health: 2000
Family & Relationships: 2000
Sports: 2000
Business & Finance: 2000
Computers & Internet: 2000
Education & Reference: 2000
Politics & Government: 2000
Entertainment & Music: 2000
Science & Mathematics: 2000
Dataset size: 20000
Train dataset size: 80.00%
Test dataset size: 10.00%
Validation dataset size: 10.00%
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=10, bias=True)
)


ZajÄ™toÅ›Ä‡ pamiÄ™ci przez model: 0.408GB
Rozmiar sÅ‚ownika: 30522
Tokenizacja zakoÅ„czona
Dodawanie etykiet zakoÅ„czone
