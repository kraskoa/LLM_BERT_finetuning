Could not locate the best model at ./results/checkpoint-500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
[34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
Test accuracy: 0.6730
Computers & Internet: 140000
Sports: 140000
Health: 140000
Business & Finance: 140000
Science & Mathematics: 140000
Entertainment & Music: 140000
Education & Reference: 140000
Family & Relationships: 140000
Politics & Government: 140000
Society & Culture: 140000
Dataset size: 1460000
Train dataset size: 95.89%
Test dataset size: 4.11%
Society & Culture: 2000
Health: 2000
Family & Relationships: 2000
Sports: 2000
Business & Finance: 2000
Computers & Internet: 2000
Education & Reference: 2000
Politics & Government: 2000
Entertainment & Music: 2000
Science & Mathematics: 2000
Dataset size: 20000
Train dataset size: 80.00%
Test dataset size: 10.00%
Validation dataset size: 10.00%
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=10, bias=True)
)


Zajƒôto≈õƒá pamiƒôci przez model: 0.408GB
Rozmiar s≈Çownika: 30522
Tokenizacja zako≈Ñczona
Dodawanie etykiet zako≈Ñczone
Test accuracy: 0.7180
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Fri May 23 15:41:50 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4060 ...    Off |   00000000:01:00.0 Off |                  N/A |
| N/A   39C    P8              3W /   55W |    6591MiB /   8188MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1736      G   /usr/lib/xorg/Xorg                        4MiB |
|    0   N/A  N/A           93063      C   ...ates/25L/LLM/.venv/bin/python       6568MiB |
+-----------------------------------------------------------------------------------------+
Computers & Internet: 140000
Sports: 140000
Health: 140000
Business & Finance: 140000
Science & Mathematics: 140000
Entertainment & Music: 140000
Education & Reference: 140000
Family & Relationships: 140000
Politics & Government: 140000
Society & Culture: 140000
Dataset size: 1460000
Train dataset size: 95.89%
Test dataset size: 4.11%
Society & Culture: 2000
Health: 2000
Family & Relationships: 2000
Sports: 2000
Business & Finance: 2000
Computers & Internet: 2000
Education & Reference: 2000
Politics & Government: 2000
Entertainment & Music: 2000
Science & Mathematics: 2000
Dataset size: 20000
Train dataset size: 80.00%
Test dataset size: 10.00%
Validation dataset size: 10.00%
Computers & Internet: 140000
Sports: 140000
Health: 140000
Business & Finance: 140000
Science & Mathematics: 140000
Entertainment & Music: 140000
Education & Reference: 140000
Family & Relationships: 140000
Politics & Government: 140000
Society & Culture: 140000
Dataset size: 1460000
Train dataset size: 95.89%
Test dataset size: 4.11%
Science & Mathematics: 5000
Health: 5000
Sports: 5000
Family & Relationships: 5000
Business & Finance: 5000
Entertainment & Music: 5000
Computers & Internet: 5000
Education & Reference: 5000
Politics & Government: 5000
Society & Culture: 5000
Dataset size: 50000
Train dataset size: 80.00%
Test dataset size: 10.00%
Validation dataset size: 10.00%
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=10, bias=True)
)


Zajƒôto≈õƒá pamiƒôci przez model: 0.408GB
Rozmiar s≈Çownika: 30522
Tokenizacja zako≈Ñczona
Dodawanie etykiet zako≈Ñczone
Dodawanie etykiet zako≈Ñczone
